# -*- coding: utf-8 -*-
"""Customer_Segmentation_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10_ppTZnV2IiUINVmWGqO6_MyFRDH3LHo
"""

import pandas as pd

data_raw = pd.read_csv('/content/Customers.csv')

data_raw.info()

data_raw.head()

data_raw.describe()

print(data_raw.isnull().sum())

import matplotlib.pyplot as plt
import seaborn as sns

sns.pairplot(data_raw.drop('customer_id', axis=1))
plt.show()

sns.heatmap(data_raw.drop('customer_id', axis=1).corr(), annot=True, cmap='viridis')
plt.show()

"""# **Conclusión:**

* las variables *monetary_purchase_amt*  y *recency_of_purchase* , tiene una  coorelacion debil y positiva, esto nos puede indicar que los clientes en promedio durante su última compra, es la de mayor monto.

* las variables *monetary_purchase_amt*  y *frequency_purchase* , tiene una  coorelacion fuerte y negativa, esto nos puede indicar que los clientes que hacen compras de montos altos, tienden a ir con menos frecuencia.

* las variables *recency_of_purchase*  y *frequency_purchase* , tiene una  coorelacion debil y positiva, esto nos puede indicar que los clientes que acuden frecuentemente , su ultima compra es mas cercana a los ultimos dias del mes.
"""

sns.boxplot(data=data_raw.drop('customer_id', axis=1))
plt.show

sns.violinplot(data=data_raw.drop('customer_id', axis=1))
plt.show

"""#IQR = Inter Quartile Range"""

import matplotlib.pyplot as plt
import sklearn
from sklearn.cluster import KMeans

#Fase de preprocesamiento
#El split:
from sklearn.model_selection import train_test_split #No lo vamos a utilizar
#Escalar los datos de la distribucion
from sklearn.preprocessing import StandardScaler

df = data_raw.drop('customer_id', axis=1)

df.info()

#Creamos dataframe escalado
##scaler = StandardScaler()
##df_scaled = scaler.fit_transform(df)



## analizando el mejor k
def best_k(df, x_col, y_col):
  sum_sq_d = []
  K = range(1,24)

  for k in K:
      km = KMeans(n_clusters=k)
      km = km.fit(df[[x_col, y_col]])
      sum_sq_d.append(km.inertia_)

  plt.figure(figsize=(8,6))

  plt.plot(K, sum_sq_d, 'rx-.')

  plt.xlabel('Number of Clusters, k', fontsize=12)
  plt.xticks(range(1,24), fontsize=12)

  plt.ylabel('Sum of Squared Distances (Distortion)', fontsize=12)
  plt.xticks(fontsize=12)

  plt.title('Elbow Method For Determining k', fontsize=16)

  plt.show()
best_k(df, 'monetary_purchase_amt', 'frequency_purchase')
best_k(df, 'frequency_purchase', 'recency_of_purchase')
best_k(df, 'monetary_purchase_amt', 'recency_of_purchase')

#Investigar como importar KMeans
#Investigar como utilizar DBScan
#Investigar como usar PCA

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA




def plot_kmeans_clustering(df, x_col, y_col, k=4, color_map=None, figsize=(5, 5)):

    if color_map is None:
        color_map = {1: 'r', 2: 'g', 3: 'b', 4: 'c', 5: 'y', 6: 'w'}

    # Crear y ajustar el modelo KMeans
    kmeans = KMeans(n_clusters=k, init='k-means++')
    kmeans.fit(df[[x_col, y_col]])

    # Obtener las etiquetas y los centroides
    labels = kmeans.predict(df[[x_col, y_col]])
    centroids = kmeans.cluster_centers_

    # Preparar los colores
    colors = [color_map[(x % len(color_map)) + 1] for x in labels]

    # Crear la gráfica
    plt.figure(figsize=figsize)
    plt.scatter(df[x_col], df[y_col], c=colors, alpha=0.4, edgecolor='k')

    for idx, centroid in enumerate(centroids):
        plt.scatter(centroid[0], centroid[1], marker='*', edgecolor='k', s=100, linewidth=2, c='black')

    plt.xlabel(x_col, fontsize=12)
    plt.xticks(fontsize=12)

    plt.ylabel(y_col, fontsize=12)
    plt.yticks(fontsize=12)

    plt.title('K-means ', fontsize=16)
    plt.show()

# Ejemplo de uso
# df es tu DataFrame
plot_kmeans_clustering(df, 'monetary_purchase_amt', 'frequency_purchase', k=3)
plot_kmeans_clustering(df, 'frequency_purchase', 'recency_of_purchase', k=3)
plot_kmeans_clustering(df, 'monetary_purchase_amt', 'recency_of_purchase', k=4)

##Usemos PCA

# Preprocesamiento con PCA
pca = PCA(n_components=2)  # Por ejemplo, reducir a 2 componentes principales
df_pca_fit = pca.fit_transform(df)

# Porcentaje de varianza explicada por cada componente principal
explained_variance_ratio = pca.explained_variance_ratio_

print("Porcentaje de Varianza Explicada por cada Componente Principal:")
print(explained_variance_ratio)

# Porcentaje de varianza total explicada
total_variance_explained = sum(explained_variance_ratio) * 100
print(f"Porcentaje de Varianza Total Explicada: {total_variance_explained:.2f}%")

# Visualización del porcentaje de varianza explicada
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center')
plt.xlabel('Componente Principal')
plt.ylabel('Porcentaje de Varianza Explicada')
plt.title('Porcentaje de Varianza Explicada por Componente Principal')
plt.show()

# Crea un DataFrame con los componentes principales
df_pca = pd.DataFrame(data=df_pca_fit, columns=['PC1', 'PC2',])

# Crear la figura y el subplot
plt.figure(figsize=(6, 4))

# Graficar PC1 vs. PC2
sns.scatterplot(x='PC1', y='PC2', data=df_pca)

# Configurar etiquetas y título
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA: PC1 vs. PC2')

# Mostrar la gráfica
plt.show()

# Aplicar k-means en los datos transformados
k = 3  # Número de clusters deseado
kmeans_pca = KMeans(n_clusters=k, init='k-means++')
kmeans_pca.fit(df_pca)

# Iterative procedure to learn labels
labels_pca = kmeans_pca.predict(df_pca)
centroids_pca = kmeans_pca.cluster_centers_

# Plot the data learned
plt.figure(figsize=(6, 6))

color_map = {1:'r', 2:'g', 3:'b', 4:'c', 5:'y', 6:'w'}
colors_pca = [color_map[x+1] for x in labels_pca]

plt.scatter(df_pca['PC1'], df_pca['PC2'], color=colors_pca, alpha=0.4, edgecolor='k')

for idx, centroid in enumerate(centroids_pca):
    plt.scatter(centroid[0],centroid[1], marker='*', edgecolor='k')

plt.xlabel('Principal Component 1', fontsize=10)
plt.ylabel('Principal Component 2', fontsize=10)
plt.title('k-means Clustering after PCA', fontsize=14)

plt.show()



# uso de DBScan
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
import seaborn as sns



def plot_clusters(data, feature1, feature2,eps_,min_samples_):



    # Definimos parámetros para DBScan

    eps = eps_  # Distancia máxima
    min_samples = min_samples_  # Número mínimo
    # modelo
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)

    # Entrenamiento
    dbscan.labels_=dbscan.fit_predict(data)

    # número clústeres encontrados
    n_clusters_ = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)



    print("Número de clústeres encontrados:", n_clusters_)
    # Porcentaje de puntos considerados como ruido por DBSCAN
    percent_noise = list(dbscan.labels_).count(-1) / len(dbscan.labels_) * 100
    print("Porcentaje de puntos considerados como ruido por DBSCAN:", percent_noise, "%")



    plt.figure(figsize=(5, 5))
    plt.scatter(data[feature1], data[feature2], c=dbscan.labels_, cmap='viridis', marker='o', edgecolors='black')
    plt.title(f'Clústeres según {feature1} y {feature2}')
    plt.xlabel(feature1)
    plt.ylabel(feature2)
    plt.grid(True)
    plt.show()

# Visualizar los clústeres
plot_clusters(df, 'monetary_purchase_amt', 'frequency_purchase', 2,10)
plot_clusters(df, 'frequency_purchase', 'recency_of_purchase', 2,10)
plot_clusters(df, 'monetary_purchase_amt', 'recency_of_purchase', 2,10)

##PCA con dbscan
plot_clusters(df_pca, 'PC1', 'PC2', 2,10)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn import metrics




# Aplicar DBSCAN
dbscan = DBSCAN(eps=2, min_samples=10)
labels = dbscan.fit_predict(df)

# Aplicar DBSCAN


# Responder las preguntas
# 1. ¿Cuántos clusters distintos de clientes se identificaron utilizando DBSCAN?
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
print("1. Número de clusters identificados:", n_clusters)

# 2. ¿Cuál es el tamaño de cada cluster?
cluster_sizes = pd.Series(labels).value_counts()
print("\n2. Tamaño de cada cluster:")
print(cluster_sizes)

# 3. ¿Cuáles son las características promedio de cada cluster?
cluster_means = df.groupby(labels).mean()
print("\n3. Características promedio de cada cluster:")
print(cluster_means)

# 4. ¿Qué cluster tiene el mayor gasto total promedio?
max_spending_cluster = cluster_means['monetary_purchase_amt'].idxmax()
print("\n4. Cluster con el mayor gasto total promedio:", max_spending_cluster)

# 5. ¿Qué cluster tiene la mayor frecuencia de compra promedio?
max_frequency_cluster = cluster_means['frequency_purchase'].idxmax()
print("\n5. Cluster con la mayor frecuencia de compra promedio:", max_frequency_cluster)

# 6. ¿Qué cluster tiene el mayor promedio de ultimo dia de compra?
max_products_cluster = cluster_means['recency_of_purchase'].idxmax()
print("\n6. Cluster con el mayor promedio de ultimo dia de compra:", max_products_cluster)



# 7. ¿Hay algún cluster que se pueda considerar como "cliente frecuente" en términos de una compra mayor de 9 millones?
frequent_customer_cluster = cluster_means[(cluster_means['frequency_purchase'] > cluster_means['frequency_purchase'].mean()) &
                                          (cluster_means['monetary_purchase_amt'] > 9)].index.tolist()
print("\n7. Cluster considerado como 'cliente frecuente' en terminos de gastos mayor a 9 millones:", frequent_customer_cluster)